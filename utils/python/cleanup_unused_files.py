#!/usr/bin/env python3
"""
Cleanup Unused Files Script
Î•Î½Ï„Î¿Ï€Î¯Î¶ÎµÎ¹ ÎºÎ±Î¹ Î¿ÏÎ³Î±Î½ÏÎ½ÎµÎ¹ Î±Ï‡ÏÎ®ÏƒÏ„Î± Î±ÏÏ‡ÎµÎ¯Î± ÏƒÏ„Î¿ trading system workspace
"""

import os
import shutil
import glob
from datetime import datetime, timedelta
from pathlib import Path
import json
import logging

# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('cleanup_report.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class FileCleanupManager:
    """Î”Î¹Î±Ï‡ÎµÎ¹ÏÎ¹ÏƒÏ„Î®Ï‚ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼Î¿Ï Î±ÏÏ‡ÎµÎ¯Ï‰Î½"""

    def __init__(self, workspace_path="."):
        self.workspace_path = Path(workspace_path)
        self.cleanup_report = {
            "timestamp": datetime.now().isoformat(),
            "actions": [],
            "statistics": {}
        }

        # Î¦Î¬ÎºÎµÎ»Î¿Î¹ Î³Î¹Î± Î¿ÏÎ³Î¬Î½Ï‰ÏƒÎ·
        self.organize_folders = {
            "logs": "logs_archive",
            "backups": "backups_archive",
            "temp": "temp_files",
            "cache": "cache_files",
            "old_configs": "old_configs"
        }

    def create_organization_folders(self):
        """Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Ï†Î±ÎºÎ­Î»Î¿Ï…Ï‚ Î¿ÏÎ³Î¬Î½Ï‰ÏƒÎ·Ï‚"""
        for folder in self.organize_folders.values():
            folder_path = self.workspace_path / folder
            folder_path.mkdir(exist_ok=True)
            logger.info(f"ğŸ“ Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î®Î¸Î·ÎºÎµ Ï†Î¬ÎºÎµÎ»Î¿Ï‚: {folder}")

    def find_unused_log_files(self):
        """Î’ÏÎ¯ÏƒÎºÎµÎ¹ Ï€Î±Î»Î¹Î¬ log Î±ÏÏ‡ÎµÎ¯Î±"""
        log_files = []
        cutoff_date = datetime.now() - timedelta(days=7)

        # Î’ÏÎµÏ‚ ÏŒÎ»Î± Ï„Î± .log Î±ÏÏ‡ÎµÎ¯Î±
        for log_file in self.workspace_path.rglob("*.log"):
            if log_file.stat().st_mtime < cutoff_date.timestamp():
                log_files.append(log_file)

        return log_files

    def find_pid_files(self):
        """Î’ÏÎ¯ÏƒÎºÎµÎ¹ PID Î±ÏÏ‡ÎµÎ¯Î±"""
        pid_files = []

        # .pid Î±ÏÏ‡ÎµÎ¯Î±
        pid_files.extend(list(self.workspace_path.rglob("*.pid")))

        # *_pid.txt Î±ÏÏ‡ÎµÎ¯Î±
        for txt_file in self.workspace_path.rglob("*_pid.txt"):
            pid_files.append(txt_file)

        return pid_files

    def find_cache_files(self):
        """Î’ÏÎ¯ÏƒÎºÎµÎ¹ cache Î±ÏÏ‡ÎµÎ¯Î±"""
        cache_files = []

        # __pycache__ Ï†Î¬ÎºÎµÎ»Î¿Î¹
        for pycache in self.workspace_path.rglob("__pycache__"):
            cache_files.append(pycache)

        # .pyc Î±ÏÏ‡ÎµÎ¯Î±
        cache_files.extend(list(self.workspace_path.rglob("*.pyc")))

        return cache_files

    def find_temporary_files(self):
        """Î’ÏÎ¯ÏƒÎºÎµÎ¹ Ï€ÏÎ¿ÏƒÏ‰ÏÎ¹Î½Î¬ Î±ÏÏ‡ÎµÎ¯Î±"""
        temp_files = []

        # Marker Î±ÏÏ‡ÎµÎ¯Î±
        temp_files.extend(list(self.workspace_path.rglob("*marker*.txt")))
        temp_files.extend(list(self.workspace_path.rglob("*offset*.txt")))

        # Backup Î±ÏÏ‡ÎµÎ¯Î± Ï€Î±Î»Î¹ÏŒÏ„ÎµÏÎ± Î±Ï€ÏŒ 30 Î·Î¼Î­ÏÎµÏ‚
        cutoff_date = datetime.now() - timedelta(days=30)
        for backup_file in self.workspace_path.rglob("*.tar.gz"):
            if backup_file.stat().st_mtime < cutoff_date.timestamp():
                temp_files.append(backup_file)

        return temp_files

    def find_duplicate_configs(self):
        """Î’ÏÎ¯ÏƒÎºÎµÎ¹ Î´Î¹Ï€Î»ÏŒÏ„Ï…Ï€Î± config Î±ÏÏ‡ÎµÎ¯Î±"""
        config_files = []

        # Î’ÏÎµÏ‚ config Î±ÏÏ‡ÎµÎ¯Î± Î¼Îµ timestamps
        for config_file in self.workspace_path.rglob("*config*.json"):
            if any(date_pattern in config_file.name for date_pattern in ["20250614", "_backup", "_old"]):
                config_files.append(config_file)

        return config_files

    def analyze_file_usage(self):
        """Î‘Î½Î±Î»ÏÎµÎ¹ Ï„Î· Ï‡ÏÎ®ÏƒÎ· Î±ÏÏ‡ÎµÎ¯Ï‰Î½"""
        analysis = {
            "unused_logs": self.find_unused_log_files(),
            "pid_files": self.find_pid_files(),
            "cache_files": self.find_cache_files(),
            "temp_files": self.find_temporary_files(),
            "duplicate_configs": self.find_duplicate_configs()
        }

        return analysis

    def safe_move_file(self, source, destination_folder):
        """ÎœÎµÏ„Î±ÎºÎ¹Î½ÎµÎ¯ Î±ÏÏ‡ÎµÎ¯Î¿ Î¼Îµ Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±"""
        try:
            dest_path = self.workspace_path / destination_folder
            dest_path.mkdir(exist_ok=True)

            dest_file = dest_path / source.name

            # Î‘Î½ Ï…Ï€Î¬ÏÏ‡ÎµÎ¹ Î®Î´Î·, Ï€ÏÏŒÏƒÎ¸ÎµÏƒÎµ timestamp
            if dest_file.exists():
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                stem = dest_file.stem
                suffix = dest_file.suffix
                dest_file = dest_path / f"{stem}_{timestamp}{suffix}"

            shutil.move(str(source), str(dest_file))

            action = {
                "action": "moved",
                "source": str(source),
                "destination": str(dest_file),
                "timestamp": datetime.now().isoformat()
            }
            self.cleanup_report["actions"].append(action)

            logger.info(f"âœ… ÎœÎµÏ„Î±ÎºÎ¹Î½Î®Î¸Î·ÎºÎµ: {source} -> {dest_file}")
            return True

        except Exception as e:
            logger.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± Î¼ÎµÏ„Î±ÎºÎ¯Î½Î·ÏƒÎ·Ï‚ {source}: {e}")
            return False

    def safe_remove_cache(self, cache_path):
        """Î”Î¹Î±Î³ÏÎ¬Ï†ÎµÎ¹ cache Î¼Îµ Î±ÏƒÏ†Î¬Î»ÎµÎ¹Î±"""
        try:
            if cache_path.is_dir():
                shutil.rmtree(cache_path)
            else:
                cache_path.unlink()

            action = {
                "action": "removed",
                "path": str(cache_path),
                "type": "cache",
                "timestamp": datetime.now().isoformat()
            }
            self.cleanup_report["actions"].append(action)

            logger.info(f"ğŸ—‘ï¸ Î”Î¹Î±Î³ÏÎ¬Ï†Î·ÎºÎµ cache: {cache_path}")
            return True

        except Exception as e:
            logger.error(f"âŒ Î£Ï†Î¬Î»Î¼Î± Î´Î¹Î±Î³ÏÎ±Ï†Î®Ï‚ cache {cache_path}: {e}")
            return False

    def organize_files(self, dry_run=True):
        """ÎŸÏÎ³Î±Î½ÏÎ½ÎµÎ¹ Ï„Î± Î±ÏÏ‡ÎµÎ¯Î±"""
        logger.info("ğŸ” Î‘Î½Î¬Î»Ï…ÏƒÎ· Î±ÏÏ‡ÎµÎ¯Ï‰Î½...")

        analysis = self.analyze_file_usage()

        # Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬
        stats = {}
        for category, files in analysis.items():
            stats[category] = len(files)

        self.cleanup_report["statistics"] = stats

        logger.info("ğŸ“Š Î£Ï„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÎ¬ Î±ÏÏ‡ÎµÎ¯Ï‰Î½:")
        for category, count in stats.items():
            logger.info(f"  {category}: {count} Î±ÏÏ‡ÎµÎ¯Î±")

        if dry_run:
            logger.info("ğŸ” DRY RUN - Î”ÎµÎ½ Î¸Î± Î³Î¯Î½Î¿Ï…Î½ Î±Î»Î»Î±Î³Î­Ï‚")
            return self.cleanup_report

        # Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï†Î±ÎºÎ­Î»Ï‰Î½ Î¿ÏÎ³Î¬Î½Ï‰ÏƒÎ·Ï‚
        self.create_organization_folders()

        # ÎœÎµÏ„Î±ÎºÎ¯Î½Î·ÏƒÎ· Î±ÏÏ‡ÎµÎ¯Ï‰Î½
        logger.info("ğŸ“¦ ÎŸÏÎ³Î¬Î½Ï‰ÏƒÎ· Î±ÏÏ‡ÎµÎ¯Ï‰Î½...")

        # Logs
        for log_file in analysis["unused_logs"]:
            self.safe_move_file(log_file, "logs_archive")

        # PID files
        for pid_file in analysis["pid_files"]:
            self.safe_move_file(pid_file, "temp_files")

        # Temp files
        for temp_file in analysis["temp_files"]:
            self.safe_move_file(temp_file, "temp_files")

        # Duplicate configs
        for config_file in analysis["duplicate_configs"]:
            self.safe_move_file(config_file, "old_configs")

        # Cache files (Î´Î¹Î±Î³ÏÎ±Ï†Î®)
        logger.info("ğŸ—‘ï¸ ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ cache...")
        for cache_file in analysis["cache_files"]:
            self.safe_remove_cache(cache_file)

        return self.cleanup_report

    def generate_report(self):
        """Î”Î·Î¼Î¹Î¿Ï…ÏÎ³ÎµÎ¯ Î±Î½Î±Ï†Î¿ÏÎ¬ ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼Î¿Ï"""
        report_file = self.workspace_path / "cleanup_report.json"

        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(self.cleanup_report, f, indent=2, ensure_ascii=False)

        logger.info(f"ğŸ“„ Î‘Î½Î±Ï†Î¿ÏÎ¬ Î±Ï€Î¿Î¸Î·ÎºÎµÏÏ„Î·ÎºÎµ: {report_file}")

        # Î£ÏÎ½Î¿ÏˆÎ·
        total_actions = len(self.cleanup_report["actions"])
        moved_files = len([a for a in self.cleanup_report["actions"] if a["action"] == "moved"])
        removed_files = len([a for a in self.cleanup_report["actions"] if a["action"] == "removed"])

        logger.info("ğŸ“‹ Î£ÏÎ½Î¿ÏˆÎ· ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼Î¿Ï:")
        logger.info(f"  Î£Ï…Î½Î¿Î»Î¹ÎºÎ­Ï‚ ÎµÎ½Î­ÏÎ³ÎµÎ¹ÎµÏ‚: {total_actions}")
        logger.info(f"  ÎœÎµÏ„Î±ÎºÎ¹Î½Î·Î¼Î­Î½Î± Î±ÏÏ‡ÎµÎ¯Î±: {moved_files}")
        logger.info(f"  Î”Î¹Î±Î³ÏÎ±Î¼Î¼Î­Î½Î± Î±ÏÏ‡ÎµÎ¯Î±: {removed_files}")

def main():
    """ÎšÏÏÎ¹Î± ÏƒÏ…Î½Î¬ÏÏ„Î·ÏƒÎ·"""
    print("ğŸ§¹ File Cleanup Manager")
    print("=" * 50)

    cleanup_manager = FileCleanupManager()

    # Î ÏÏÏ„Î± ÎºÎ¬Î½Îµ dry run
    print("\nğŸ” Î‘Î½Î¬Î»Ï…ÏƒÎ· Î±ÏÏ‡ÎµÎ¯Ï‰Î½ (Dry Run)...")
    report = cleanup_manager.organize_files(dry_run=True)

    # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· ÏƒÏ„Î±Ï„Î¹ÏƒÏ„Î¹ÎºÏÎ½
    print("\nğŸ“Š Î’ÏÎ­Î¸Î·ÎºÎ±Î½:")
    for category, count in report["statistics"].items():
        print(f"  {category}: {count} Î±ÏÏ‡ÎµÎ¯Î±")

    # Î•ÏÏÏ„Î·ÏƒÎ· Î³Î¹Î± ÎµÎºÏ„Î­Î»ÎµÏƒÎ·
    response = input("\nâ“ Î˜Î­Î»ÎµÏ„Îµ Î½Î± Ï€ÏÎ¿Ï‡Ï‰ÏÎ®ÏƒÎµÏ„Îµ Î¼Îµ Ï„Î·Î½ Î¿ÏÎ³Î¬Î½Ï‰ÏƒÎ·; (y/N): ")

    if response.lower() in ['y', 'yes', 'Î½Î±Î¹', 'Î½']:
        print("\nğŸš€ Î•ÎºÏ„Î­Î»ÎµÏƒÎ· ÎºÎ±Î¸Î±ÏÎ¹ÏƒÎ¼Î¿Ï...")
        cleanup_manager.organize_files(dry_run=False)
        cleanup_manager.generate_report()
        print("\nâœ… ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î¿Î»Î¿ÎºÎ»Î·ÏÏÎ¸Î·ÎºÎµ!")
    else:
        print("\nâŒ ÎšÎ±Î¸Î±ÏÎ¹ÏƒÎ¼ÏŒÏ‚ Î±ÎºÏ…ÏÏÎ¸Î·ÎºÎµ")

if __name__ == "__main__":
    main()